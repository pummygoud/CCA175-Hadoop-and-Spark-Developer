Instructions
Connect to the MySQL database on the itversity labs using sqoop and import all of the data from the orders table into HDFS

Data Description
1.A MySQL instance is running on a remote node ms.itversity.com in the instance. You will find a table that contains 68883 rows of orders data

MySQL database information:

Installation on the node ms.itversity.com
Database name is retail_db
Username: retail_user
Password: itversity
Table name orders
Output Requirements
Place the customer files in the HDFS directory
/user/`whoami`/problem1/solution/
Replace `whoami` with your OS user name
Use a text format with comma as the columnar delimiter
Load every order record completely


sqoop-import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--target-dir /user/shwetatanwar13/problem1/solution/ \
--as-textfile \
--fields-terminated-by ','

hdfs dfs -ls /user/shwetatanwar13/problem1/solution/



sqoop-eval \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--query "select count(*) from orders"

2.Instructions
Get the customers who have not placed any orders, sorted by customer_lname and then customer_fname

Data Description
Data is available in local file system /data/retail_db

retail_db information:

Source directories: /data/retail_db/orders and /data/retail_db/customers
Source delimiter: comma(",")
Source Columns - orders - order_id, order_date, order_customer_id, order_status
Source Columns - customers - customer_id, customer_fname, customer_lname and many more
Output Requirements
Target Columns: customer_lname, customer_fname
Number of Files: 1
Place the output file in the HDFS directory
/user/`whoami`/problem2/solution/
Replace `whoami` with your OS user name
File format should be text
delimiter is (",")
Compression: Uncompressed

hdfs dfs -put /data/retail_db/orders /user/shwetatanwar13/simulator
hdfs dfs -put /data/retail_db/customers /user/shwetatanwar13/simulator

ordersdf=sc.textFile("/user/shwetatanwar13/simulator/orders"). \
map(lambda x:x.split(',')).toDF(["order_id","order_date","order_customer_id","order_status"])

custDF=sc.textFile("/user/shwetatanwar13/simulator/customers"). \
map(lambda x:x.split(',')[0:3]).toDF(["customer_id","customer_fname","customer_lname"])

ordersdf.registerTempTable("orders_sim")
custDF.registerTempTable("cust_sim")

res=sqlContext.sql("select concat(customer_lname,',',customer_fname) \
from cust_sim c left outer join orders_sim o \
on o.order_customer_id = c.customer_id \
where o.order_id is null")

res.coalesce(1).write.text("/user/shwetatanwar13/problem2/solution/")
hdfs dfs -ls /user/shwetatanwar13/problem2/solution/


3.Instructions
Get top 3 crime types based on number of incidents in RESIDENCE area using "Location Description"

Data Description
Data is available in HDFS under /public/crime/csv

crime data information:

Structure of data: (ID, Case Number, Date, Block, IUCR, Primary Type, Description, Location Description, Arrst, Domestic, Beat, District, Ward, Community Area, FBI Code, X Coordinate, Y Coordinate, Year, Updated on, Latitude, Longitude, Location)
File format - text file
Delimiter - "," (use regex while splitting split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1), as there are some fields with comma and enclosed using double quotes.
Output Requirements
Output Fields: crime_type, incident_count
Output File Format: JSON
Delimiter: N/A
Compression: No
Place the output file in the HDFS directory
/user/`whoami`/problem3/solution/
Replace `whoami` with your OS user name
End of Problem

rdd=sc.textFile("/public/crime/csv").map(lambda x:(x.split(',')[5],1))

final=rdd.reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(ascending=False).map(lambda x:(x[1],x[0]))

final=final.filter(lambda x:x[1]>=734958).map(lambda x:x[0]+'\t'+str(x[1]))
final.saveAsTextFile("/user/shwetatanwar13//problem3/solution/")

hdfs dfs -ls /user/shwetatanwar13//problem3/solution/

*******************************************************************************

4.Instructions
Convert NYSE data into parquet

NYSE data Description
Data is available in local file system under /data/NYSE (ls -ltr /data/NYSE)

NYSE Data information:

Fields (stockticker:string, transactiondate:string, openprice:float, highprice:float, lowprice:float, closeprice:float, volume:bigint)
Output Requirements
Column Names: stockticker, transactiondate, openprice, highprice, lowprice, closeprice, volume
Convert file format to parquet
Place the output file in the HDFS directory
/user/`whoami`/problem4/solution/
Replace `whoami` with your OS user name
End of Problem

hdfs dfs -put /data/nyse /user/shwetatanwar13/simulator/

hdfs dfs -ls /user/shwetatanwar13/simulator/

df=sc.textFile("/user/shwetatanwar13/simulator/nyse").map(lambda x:Row(stockticker=x.split(',')[0],transactiondate=x.split(',')[1],openprice=float(x.split(',')[2]),highprice=x.split(',')[3], \
lowprice=x.split(',')[4],closeprice=float(x.split(',')[5]),volume=int(x.split(',')[6]))). \
toDF()

df=sc.textFile("/user/shwetatanwar13/simulator/nyse").map(lambda x:(x.split(',')[0],x.split(',')[1],float(x.split(',')[2]),float(x.split(',')[3]),float(x.split(',')[4]),float(x.split(',')[5]),x.split(',')[6])).toDF(["stockticker","transactiondate","openprice", "highprice","lowprice","closeprice","volume"])

df.write.parquet("/user/shwetatanwar13/problem4/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem4/solution/

5.Instructions
Get word count for the input data using space as delimiter (for each word, we need to get how many times it is repeated in the entire input data set)

Data Description
Data is available in HDFS /public/randomtextwriter

word count data information:

Number of executors should be 10
executor memory should be 3 GB
Executor cores should be 20 in total (2 per executor)
Number of output files should be 8
Avro dependency details: groupId -> com.databricks, artifactId -> spark-avro_2.10, version -> 2.0.1
Output Requirements
Output File format: Avro
Output fields: word, count
Compression: Uncompressed
Place the customer files in the HDFS directory
/user/`whoami`/problem5/solution/
Replace `whoami` with your OS user name
End of Problem

pyspark --master yarn --num-executors 10 --executor-cores 2 --executor-memory 3G --packages com.databricks:spark-avro_2.10:2.0.1

rdd=sc.sequenceFile("/public/randomtextwriter").map(lambda x:x[0]+x[1]).flatMap(lambda x:x.split()).map(lambda x:(x,1))

rdd=rdd.coalesce(20).reduceByKey(lambda x,y:x+y)

df=rdd.toDF(["word","count"])
df.coalesce(8).write.format("com.databricks.spark.avro").save("/user/shwetatanwar13/problem5/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem5/solution/

6.Instructions
Get total number of orders for each customer where the cutomer_state = 'TX'

Data Description
retail_db data is available in HDFS at /public/retail_db

retail_db data information:

Source directories: /public/retail_db/orders and /public/retail_db/customers
Source Columns - orders - order_id, order_date, order_customer_id, order_status
Source Columns - customers - customer_id, customer_fname, customer_lname, customer_state (8th column) and many more
delimiter: (",")
Output Requirements
Output Fields: customer_fname, customer_lname, order_count
File Format: text
Delimiter: Tab character (\t)
Place the result file in the HDFS directory
/user/`whoami`/problem6/solution/
Replace `whoami` with your OS user name
End of Problem


ordersdf=sc.textFile("/public/retail_db/orders").map(lambda x:(int(x.split(',')[0]),x.split(',')[1],int(x.split(',')[2]),x.split(',')[3])) \
.toDF(["order_id","order_date","order_customer_id","order_status"])

customerdf=sc.textFile("/public/retail_db/customers").map(lambda x:(int(x.split(',')[0]),x.split(',')[1],x.split(',')[2],x.split(',')[7])).toDF(["customer_id","customer_fname","customer_lname","customer_state"])
ordersdf.registerTempTable("orders6")
customerdf.registerTempTable("customers6")

res=sqlContext.sql("select concat(customer_fname,'\t',customer_lname,'\t',count(1)) \
from orders6 o join customers6 c \
on o.order_customer_id = c.customer_id \
where customer_state = 'TX' \
group by customer_fname,customer_lname")

res.write.text("/user/shwetatanwar13/problem6/solution/")

7.Instructions
List the names of the Top 5 products by revenue ordered on '2013-07-26'. Revenue is considered only for COMPLETE and CLOSED orders.

Data Description
retail_db data is available in HDFS at /public/retail_db

retail_db data information:

Source directories: 
/public/retail_db/orders 
/public/retail_db/order_items 
/public/retail_db/products
Source delimiter: comma(",")
Source Columns - orders - order_id, order_date, order_customer_id, order_status
Source Columns - order_itemss - order_item_id, order_item_order_id, order_item_product_id, order_item_quantity, order_item_subtotal, order_item_product_price
Source Columns - products - product_id, product_category_id, product_name, product_description, product_price, product_image
Output Requirements
Target Columns: order_date, order_revenue, product_name, product_category_id
Data has to be sorted in descending order by order_revenue
File Format: text
Delimiter: colon (:)
Place the output file in the HDFS directory
/user/`whoami`/problem7/solution/
Replace `whoami` with your OS user name
End of Problem

ordf=sc.textFile("/public/retail_db/orders").map(lambda x:(int(x.split(',')[0]),x.split(',')[1],int(x.split(',')[2]),x.split(',')[3])).toDF(["order_id","order_date","order_customer_id", "order_status"])

oidf=sc.textFile("/public/retail_db/order_items").map(lambda x:(int(x.split(',')[0]),int(x.split(',')[1]),int(x.split(',')[2]),int(x.split(',')[3]),float(x.split(',')[4]),float(x.split(',')[5]))).toDF(["order_item_id","order_item_order_id","order_item_product_id","order_item_quantity","order_item_subtotal","order_item_product_price"])


pdf=sc.textFile("/public/retail_db/products").filter(lambda x:x.split(',')[4]!='').map(lambda x:(int(x.split(',')[0]),int(x.split(',')[1]),x.split(',')[2],x.split(',')[3],float(x.split(',')[4]),x.split(',')[5])).toDF(["product_id", "product_category_id","product_name","product_description","product_price","product_image"])

ordf.registerTempTable("orders7")
oidf.registerTempTable("order_items7")
pdf.registerTempTable("products7")

sqlContext.sql("SET spark.sql.shuffle.partitions = 10")

res=sqlContext.sql("select concat_ws(',',order_date,order_revenue, product_name, \
product_category_id) from(select order_date, sum(order_item_subtotal) order_revenue, product_name, \
product_category_id,rank() over (order by sum(order_item_subtotal) desc) rank  \
from orders7 o join order_items7 oi \
on o.order_id=oi.order_item_order_id \
join products7 p \
on p.product_id=oi.order_item_product_id \
where order_date like '2013-07-26%' \
and order_status in ('COMPLETE','CLOSED') \
group by order_date,product_name, product_category_id)q \
where rank <=5 \
order by order_revenue desc")


sqlContext.sql("Select * from products7 where product_price is null").show()

res.coalesce(4).write.text("/user/shwetatanwar13/problem7/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem7/solution/

8.Instructions
List the order Items where the order_status = 'PENDING PAYMENT' order by order_id

Data Description
Data is available in HDFS location

retail_db data information:

Source directories: /data/retail_db/orders
Source delimiter: comma(",")
Source Columns - orders - order_id, order_date, order_customer_id, order_status
Output Requirements
Target columns: order_id, order_date, order_customer_id, order_status
File Format: orc
Place the output files in the HDFS directory
/user/`whoami`/problem8/solution/
Replace `whoami` with your OS user name
End of Problem

hdfs dfs -put /data/retail_db/orders /user/shwetatanwar13/simulator/problem8

orderdf=sc.textFile("/user/shwetatanwar13/simulator/problem8/orders").map(lambda x:x.split(',')).toDF(["order_id","order_date", "order_customer_id","order_status"])

orderdf.registerTempTable("orders8")

res=sqlContext.sql("select order_id, order_date,order_customer_id,order_status \
from orders8 o \
where order_status = 'PENDING_PAYMENT' \
order by order_id")

sqlContext.sql("select distinct(order_status) from orders8")

res.write.orc("/user/shwetatanwar13/problem8/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem8/solution/


***********************************13JAn19***************************************************

1.Instructions
Remove header from h1b data

Data Description
h1b data with ascii null "\0" as delimiter is available in HDFS

h1b data information:

HDFS location: /public/h1b/h1b_data
First record is the header for the data
Output Requirements
Remove the header from the data and save rest of the data as is
Data should be compressed using snappy algorithm
Place the H1B data in the HDFS directory
/user/`whoami`/problem9/solution/
Replace `whoami` with your OS user name
End of Problem

hdfs dfs -ls /public/h1b/h1b_data

rdd=sc.textFile("/public/h1b/h1b_data")

f=rdd.first()

rdd=rdd.filter(lambda x:x!=f)

rdd.saveAsTextFile("/user/shwetatanwar13/problem9/solution/",CompressionCodecClass="org.apache.hadoop.io.compress.SnappyCodec")

hdfs dfs -ls /user/shwetatanwar13/problem9/solution/

2.Instructions
Get number of LCAs filed for each year

Data Description
h1b data with ascii null "\0" as delimiter is available in HDFS

h1b data information:

HDFS Location: /public/h1b/h1b_data
Ignore first record which is header of the data
YEAR is 8th field in the data
There are some LCAs for which YEAR is NA, ignore those records
Output Requirements
File Format: text
Output Fields: YEAR, NUMBER_OF_LCAS
Delimiter: Ascii null "\0"
Place the output files in the HDFS directory
/user/`whoami`/problem10/solution/
Replace `whoami` with your OS user name
End of Problem


rdd=sc.textFile("/public/h1b/h1b_data")

f=rdd.first()

rdd=rdd.filter(lambda x:x!=f)

rdd=rdd.filter(lambda x:x.split('\0')[7]!='NA')

finalrdd=rdd.map(lambda x:(x.split('\0')[7],1)).reduceByKey(lambda x,y:x+y)

finalrdd.map(lambda x:x[0]+'\0'+str(x[1])).saveAsTextFile("/user/shwetatanwar13/problem10/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem10/solution/

3.Instructions
Get number of LCAs by status for the year 2016

Data Description
h1b data with ascii null "\0" as delimiter is available in HDFS

h1b data information:

HDFS Location: /public/h1b/h1b_data
Ignore first record which is header of the data
YEAR is 8th field in the data
STATUS is 2nd field in the data
There are some LCAs for which YEAR is NA, ignore those records
Output Requirements
File Format: json
Output Field Names: year, status, count
Place the output files in the HDFS directory
/user/`whoami`/problem11/solution/
Replace `whoami` with your OS user name
End of Problem


rdd=sc.textFile("/public/h1b/h1b_data")
f=rdd.first()
rdd=rdd.filter(lambda x:x!=f)
rdd=rdd.filter(lambda x:x.split('\0')[7]!='NA' and x.split('\0')[7]=='2016')

statusrdd=rdd.map(lambda x:((x.split('\0')[7],x.split('\0')[1]),1)).reduceByKey(lambda x,y:x+y)


df=statusrdd.map(lambda x:(x[0][0],x[0][1],x[1])).toDF(["year","status","count"])
df.write.json("/user/shwetatanwar13/problem11/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem11/solution/


4.Instructions
Get top 5 employers for year 2016 where the status is WITHDRAWN or CERTIFIED-WITHDRAWN or DENIED

Data Description
h1b data with ascii null "\0" as delimiter is available in HDFS

h1b data information:

HDFS Location: /public/h1b/h1b_data
Ignore first record which is header of the data
YEAR is 7th field in the data
STATUS is 2nd field in the data
EMPLOYER is 3rd field in the data
There are some LCAs for which YEAR is NA, ignore those records
Output Requirements
File Format: parquet
Output Fields: employer_name, lca_count
Data needs to be in descending order by count
Place the output files in the HDFS directory
/user/`whoami`/problem12/solution/
Replace `whoami` with your OS user name
End of Problem


rdd=sc.textFile("/public/h1b/h1b_data")
f=rdd.first()
rdd=rdd.filter(lambda x:x!=f)
rdd=rdd.filter(lambda x:x.split('\0')[7]!='NA')

df=rdd.map(lambda x:(x.split('\0')[7],x.split('\0')[1],x.split('\0')[2])).toDF(["year","status","employer"])

df.registerTempTable("h1b_4")

res=sqlContext.sql("select concat(employer,',',count(*)) \
from h1b_4 \
where status in ('WITHDRAWN','CERTIFIED-WITHDRAWN','DENIED') \
and year ='2016' \
group by employer \
order by count(*) desc \
limit 5")

res.write.text("/user/shwetatanwar13/problem12/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem12/solution/

5.Instructions
Copy all h1b data from HDFS to Hive table excluding those where year is NA or prevailing_wage is NA

Data Description
h1b data with ascii null "\0" as delimiter is available in HDFS

h1b data information:

HDFS Location: /public/h1b/h1b_data_noheader
Fields: 
ID, CASE_STATUS, EMPLOYER_NAME, SOC_NAME, JOB_TITLE, FULL_TIME_POSITION, PREVAILING_WAGE, YEAR, WORKSITE, LONGITUDE, LATITUDE
Ignore data where PREVAILING_WAGE is NA or YEAR is NA
PREVAILING_WAGE is 7th field
YEAR is 8th field
Number of records matching criteria: 3002373
Output Requirements
Save it in Hive Database
Create Database: CREATE DATABASE IF NOT EXISTS shwetatanwar13
Switch Database: USE shwetatanwar13
Save data to hive table h1b_data
Create table command:

CREATE TABLE h1b_data (
  ID                 INT,
  CASE_STATUS        STRING,
  EMPLOYER_NAME      STRING,
  SOC_NAME           STRING,
  JOB_TITLE          STRING,
  FULL_TIME_POSITION STRING,
  PREVAILING_WAGE    DOUBLE,
  YEAR               INT,
  WORKSITE           STRING,
  LONGITUDE          STRING,
  LATITUDE           STRING
)
                
Replace `whoami` with your OS user name
End of Problem

x='ID,CASE_STATUS,EMPLOYER_NAME,SOC_NAME,JOB_TITLE,FULL_TIME_POSITION,PREVAILING_WAGE,YEAR,WORKSITE,LONGITUDE,LATITUDE'.split(',')

rdd=sc.textFile("/public/h1b/h1b_data_noheader").filter(lambda x:x.split('\0')[7]!='NA' and x.split('\0')[6]!='NA')

df=rdd.map(lambda x:x.split('\0')).map(lambda x:(int(x[0]),x[1],x[2],x[3],x[4],x[5],float(x[6]),int(x[7]),x[8],x[9],x[10])).toDF(x)

df.insertInto('shwetatanwar13.h1b_data')

6.Instructions
Export h1b data from hdfs to MySQL Database

Data Description
h1b data with ascii character "\001" as delimiter is available in HDFS

h1b data information:

HDFS Location: /public/h1b/h1b_data_to_be_exported
Fields: 
ID, CASE_STATUS, EMPLOYER_NAME, SOC_NAME, JOB_TITLE, FULL_TIME_POSITION, PREVAILING_WAGE, YEAR, WORKSITE,LONGITUDE, LATITUDE
Number of records: 3002373
Output Requirements
Export data to MySQL Database
MySQL database is running on ms.itversity.com
User: h1b_user
Password: itversity
Database Name: h1b_export
Table Name: h1b_data_shwetatanwar13
Nulls are represented as: NA
After export nulls should not be stored as NA in database. It should be represented as database null
Create table command:

CREATE TABLE h1b_data_shwetatanwar13 (
  ID                 INT, 
  CASE_STATUS        VARCHAR(50), 
  EMPLOYER_NAME      VARCHAR(100), 
  SOC_NAME           VARCHAR(100), 
  JOB_TITLE          VARCHAR(100), 
  FULL_TIME_POSITION VARCHAR(50), 
  PREVAILING_WAGE    FLOAT, 
  YEAR               INT, 
  WORKSITE           VARCHAR(50), 
  LONGITUDE          VARCHAR(50), 
  LATITUDE           VARCHAR(50));
                
Replace `whoami` with your OS user name
Above create table command can be run using
Login using mysql -u h1b_user -h ms.itversity.com -p
When prompted enter password itversity
Switch to database using use h1b_export
Run above create table command by replacing `whoami` with your OS user name
End of Problem

sqoop-export \
--connect jdbc:mysql://ms.itversity.com:3306/h1b_export \
--username h1b_user \
--password itversity \
--table h1b_data_shwetatanwar13 \
--export-dir /public/h1b/h1b_data_to_be_exported \
--input-fields-terminated-by '\001' \
--input-null-string 'NA'

select count(*) from h1b_data_shwetatanwar13;

7.Instructions
Connect to the MySQL database on the itversity labs using sqoop and import data with case_status as CERTIFIED

Data Description
A MySQL instance is running on a remote node ms.itversity.com in the instance. You will find a table that contains 3002373 rows of h1b data

MySQL database information:

Installation on the node ms.itversity.com
Database name is h1b_db
Username: h1b_user
Password: itversity
Table name h1b_data

Output Requirements
Place the h1b related data in files in HDFS directory
/user/`whoami`/problem15/solution/
Replace `whoami` with your OS user name
Use avro file format
Load only those records which have case_status as CERTIFIED completely
There are 2615623 such records
End of Problem

sqoop-import \
--connect jdbc:mysql://ms.itversity.com:3306/h1b_db \
--username h1b_user \
--password itversity \
--table h1b_data \
--target-dir /user/shwetatanwar13/problem15/solution/ \
--as-avrodatafile \
--where "case_status='CERTIFIED'"

hdfs dfs -ls /user/shwetatanwar13/problem15/solution/

8.Instructions
Get NYSE data in ascending order by date and descending order by volume

Data Description
NYSE data with "," as delimiter is available in HDFS

NYSE data information:

HDFS location: /public/nyse
There is no header in the data
Output Requirements
Save data back to HDFS
Column order: stockticker,transactiondate,openprice,highprice,lowprice,closeprice,volume
File Format: text
Delimiter: :
Place the sorted NYSE data in the HDFS directory
/user/`whoami`/problem16/solution/
Replace `whoami` with your OS user name
End of Problem

hdfs dfs -ls /public/nyse

rdd=sc.textFile("/public/nyse")

final=rdd.map(lambda x:((x.split(',')[1],-int(x.split(',')[6])),x)).sortByKey()

final.map(lambda x:x[1]).saveAsTextFile("/user/shwetatanwar13/problem16/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem16/solution/

9.Instructions
Get the stock tickers from NYSE data for which full name is missing in NYSE symbols data

Data Description
NYSE data with "," as delimiter is available in HDFS

stockticker,transactiondate,openprice,highprice,lowprice,closeprice,volume

NYSE data information:

HDFS location: /public/nyse
There is no header in the data

NYSE Symbols data with " " as delimiter is available in HDFS

NYSE Symbols data information:

HDFS location: /public/nyse_symbols
First line is header and it should be included

Output Requirements
Get unique stock ticker for which corresponding names are missing in NYSE symbols data
Save data back to HDFS
File Format: avro
Avro dependency details: 
groupId -> com.databricks, artifactId -> spark-avro_2.10, version -> 2.0.1
Place the sorted NYSE data in the HDFS directory
/user/`whoami`/problem17/solution/
Replace `whoami` with your OS user name
End of Problem

nyse=sc.textFile("/public/nyse").map(lambda x:(x.split(',')[0],x))
nyse_sym=sc.textFile("/public/nyse_symbols").map(lambda x:(x.split('\t')[0],x.split('\t')[1]))

joinrdd=nyse.join(nyse_sym)

finalrdd=joinrdd.filter(lambda x:x[1][1]==None).map(lambda x:x[0]).distinct()

finalrdd.toDF().write.format("com.databricks.spark.avro").save("/user/shwetatanwar13/problem17/solution/")

10.Instructions
Get the name of stocks displayed along with other information

Data Description
NYSE data with "," as delimiter is available in HDFS

NYSE data information:

HDFS location: /public/nyse
There is no header in the data
NYSE Symbols data with tab character (\t) as delimiter is available in HDFS

NYSE Symbols data information:

HDFS location: /public/nyse_symbols
First line is header and it should be included
Output Requirements
Get all NYSE details along with stock name if exists, if not stockname should be empty
Column Order: stockticker, stockname, transactiondate, openprice, highprice, lowprice, closeprice, volume
Delimiter: ,
File Format: text
Place the data in the HDFS directory
/user/`whoami`/problem18/solution/
Replace `whoami` with your OS user name
End of Problem

	nyse=sc.textFile("/public/nyse").map(lambda x:(x.split(',')[0],x))
	nyse_sym=sc.textFile("/public/nyse_symbols").map(lambda x:(x.split('\t')[0],x.split('\t')[1]))

joinrdd=nyse.join(nyse_sym)

final=joinrdd.map(lambda x:x[1][0].split(',')[0]+','+x[1][1]+','+','.join(i for i in x[1][0].split(',')[1:7]))

final.saveAsTextFile("/user/shwetatanwar13/problem18/solution/")
hdfs dfs -ls /user/shwetatanwar13/problem18/solution/


19.Instructions
Get number of companies who filed LCAs for each year

Data Description
h1b data with ascii null "\0" as delimiter is available in HDFS

h1b data information:

HDFS Location: /public/h1b/h1b_data_noheader
Fields: 
ID, CASE_STATUS, EMPLOYER_NAME, SOC_NAME, JOB_TITLE, FULL_TIME_POSITION, PREVAILING_WAGE, YEAR, WORKSITE, LONGITUDE, LATITUDE
Use EMPLOYER_NAME as the criteria to identify the company name to get number of companies
YEAR is 8th field
There are some LCAs for which YEAR is NA, ignore those records
Output Requirements
File Format: text
Delimiter: tab character "\t"
Output Field Order: year, lca_count
Place the output files in the HDFS directory
/user/`whoami`/problem19/solution/
Replace `whoami` with your OS user name
End of Problem

rdd=sc.textFile("/public/h1b/h1b_data_noheader").map(lambda x:(x.split('\0')[7],x.split('\0')[2]))

final=rdd.groupByKey().map(lambda x:x[0]+'\t'+str(len(set(x[1]))))

final.saveAsTextFile("/user/shwetatanwar13/problem19/solution/")

hdfs dfs -ls /user/shwetatanwar13/problem19/solution/

20.Instructions
Connect to the MySQL database on the itversity labs using sqoop and import data with employer_name, case_status and count. Make sure data is sorted by employer_name in ascending order and by count in descending order

Data Description
A MySQL instance is running on a remote node ms.itversity.com in the instance. You will find a table that contains 3002373 rows of h1b data

MySQL database information:

Installation on the node ms.itversity.com
Database name is h1b_db
Username: h1b_user
Password: itversity
Table name h1b_data
Output Requirements
Place the h1b related data in files in HDFS directory
/user/`whoami`/problem20/solution/
Replace `whoami` with your OS user name
Use text file format and tab (\t) as delimiter
Hint: You can use Spark with JDBC or Sqoop import with query
You might not get such hints in actual exam
Output should contain employer name, case status and count
End of Problem

sqoop-import \
--connect jdbc:mysql://ms.itversity.com:3306/h1b_db \
--username h1b_user \
--password itversity \
--target-dir /user/shwetatanwar13/problem20/solution/ \
--fields-terminated-by '\t' \
--as-textfile \
--query "select employer_name,case_status,count(*) count
from h1b_data where \$CONDITIONS group by employer_name,case_status order by employer_name desc,count desc" \
-m 1

hdfs dfs -ls /user/shwetatanwar13/problem20/solution/



