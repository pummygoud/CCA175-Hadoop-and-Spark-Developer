QUESTION 18 CORRECT TEXT Problem Scenario 27 : 
You need to implement near real time solutions for collecting information when submitted in file with below information. 
Data 
echo "IBM,100,20160104" >> /tmp/spooldir/bb/.bb.txt 
echo "IBM,103,20160105" >> /tmp/spooldir/bb/.bb.txt 
mv /tmp/spooldir/bb/.bb.txt /tmp/spooldir/bb/bb.txt 

After few mins 
echo "IBM,100.2,20160104" >> /tmp/spooldir/dr/.dr.txt 
echo "IBM,103.1,20160105" >> /tmp/spooldir/dr/.dr.txt 
mv /tmp/spooldir/dr/.dr.txt /tmp/spooldir/dr/dr.txt 

Requirements: 

You have been given below directory location (if not available than create it) 
/tmp/spooldir . 
You have a finacial subscription for getting stock prices from BloomBerg as well as Reuters and using ftp you download every hour new files from their respective ftp site in directories /tmp/spooldir/bb and /tmp/spooldir/dr respectively. 
As soon as file committed in this directory that needs to be available in hdfs in /tmp/flume/finance location in a single directory. 

Write a flume configuration file named flume7.conf and use it to load data in hdfs with following additional properties . 
1. Spool /tmp/spooldir/bb and /tmp/spooldir/dr 
2. File prefix in hdfs should be events 
3. File suffix should be .log 
4. If file is not commited and in use than it should have _ as prefix. 
5. Data should be written as text to hdfs

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1 r2
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /tmp/spooldir/bb
a1.sources.r2.type = spooldir
a1.sources.r2.spoolDir = /tmp/spooldir/dr


# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /tmp/flume/finance
a1.sinks.k1.hdfs.filePrefix = events
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.inUsePrefix = _
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text

# Use a channel which buffers events in memory
a1.channels.c1.type = file

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sources.r2.channels = c1
a1.sinks.k1.channel = c1

#use deletePolicy to delete spool dir files after reading

flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob1.conf


QUESTION 32 CORRECT TEXT Problem Scenario 25 : 
You have been given below comma separated employee information. 
That needs to be added in /home/cloudera/flumetest/in.txt file (to do tail source) 
sex,name,city 
1,alok,mumbai 
1,jatin,chennai 
1,yogesh,kolkata 
2,ragini,delhi 
2,jyotsana,pune 
1,valmiki,banglore 

Create a flume conf file using fastest non-durable channel, which write data in hive warehouse directory, in two separate tables called flumemaleemployee1 and flumefemaleemployee1 (Create hive table as well for given data}. 

Please use tail source with /home/cloudera/flumetest/in.txt file. 
Flumemaleemployee1 : will contain only male employees data 
flumefemaleemployee1 : Will contain only woman employees data



agent.sources = tailsrc 
agent.channels = mem1 mem2 
agent.sinks = stdl std2
 
agent.sources.tailsrc.type = exec 
agent.sources.tailsrc.command = tail -F /home/cloudera/flumetest/in.txt agent.sources.tailsrc.batchSize = 1 
agent.sources.tailsrc.interceptors = i1 
agent.sources.tailsrc.interceptors.i1.type = regex_extractor agent.sources.tailsrc.interceptors.il.regex = ^(\\d)
agent.sources.tailsrc.interceptors.i1.serializers = t1 
agent.sources.tailsrc.interceptors.i1.serializers.t1.name = type agent.sources.tailsrc.selector.type = multiplexing 
agent.sources.tailsrc.selector.header = type 
agent.sources.tailsrc.selector.mapping.1 = mem1 
agent.sources.tailsrc.selector.mapping.2 = mem2

agent.sinks.std1.type = hdfs 
agent.sinks.stdl.channel = mem1 
agent.sinks.stdl.batchSize = 1 
agent.sinks.std1.hdfs.path = /user/hive/warehouse/flumemaleemployee1 
agent.sinks.stdl.rolllnterval = 0 
agent.sinks.stdl.hdfs.FileType = DataStream 
agent.sinks.std2.type = hdfs 
agent.sinks.std2.channel = mem2 
agent.sinks.std2.batchSize = 1 
agent.sinks.std2.hdfs.path = /user/hive/warehouse/fIumefemaleemployee1 agent.sinks.std2.rolllnterval = 0 
agent.sinks.std2.hdfs.FileType = DataStream 

agent.channels.mem1.type = memory 
agent.channels.meml.capacity = 100 
agent.channels.mem2.type = memory 
agent.channels.mem2.capacity = 100 
agent.sources.tailsrc.channels = mem1 mem2 

/user/hive/warehouse/flumemaleemployee1
/user/hive/warehouse/flumefemaleemployee1

flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob2.conf
hdfs dfs -ls /user/hive/warehouse/flumefemaleemployee1

hdfs dfs -ls /user/hive/warehouse/flumemaleemployee1

agent1.sources.tailsrc.selector.type = multiplexing 
agent1.sources.tailsrc.selector.header = type1 
agent1.sources.tailsrc.selector.mapping.1 = mem1 
agent1.sources.tailsrc.selector.mapping.2 = mem2


create table Flumemaleemployee1(
sex int,
name string,
city string
)
row format delimited fields terminated by ','
location '/user/hive/warehouse/flumemaleemployee1';

create table Flumefemaleemployee1(
sex int,
name string,
city string
)
row format delimited fields terminated by ','
location '/user/hive/warehouse/flumefemaleemployee1';




QUESTION 38 CORRECT TEXT Problem Scenario 26 : You need to implement near real time solutions for collecting information when submitted in file with below information. 
You have been given below directory location (if not available than create it) /tmp/nrtcontent. 
Assume your departments upstream service is continuously committing data in this directory as a new file (not stream of data, because it is near real time solution). 
As soon as file committed in this directory that needs to be available in hdfs in /tmp/flume location 
Data 
echo "I am preparing for CCA175 from ABCTECH.com" > /tmp/nrtcontent/.he1.txt 
mv /tmp/nrtcontent/.he1.txt /tmp/nrtcontent/he1.txt 

After few mins 
echo "I am preparing for CCA175 from TopTech.com" > /tmp/nrtcontent/.qt1.txt 

mv /tmp/nrtcontent/.qt1.txt /tmp/nrtcontent/qt1.txt 

Write a flume configuration file named flumes.conf and use it to load data in hdfs with following additional properties. 
1. Spool /tmp/nrtcontent 
2. File prefix in hdfs should be events 
3. File suffix should be .log 
4. If file is not commited and in use than it should have as prefix. 
5. Data should be written as text to hdfs 


# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /tmp/nrtcontent

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /tmp/flume/nrtcontent
a1.sinks.k1.hdfs.filePrefix = events
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.inUsePrefix = _
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text

# Use a channel which buffers events in memory
a1.channels.c1.type = file

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sources.r2.channels = c1
a1.sinks.k1.channel = c1


flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob3.conf

QUESTION 48 CORRECT TEXT Problem Scenario 21 : You have been given log generating service as below. 
startjogs (It will generate continuous logs) 
tailjogs (You can check , what logs are being generated) 
stopjogs (It will stop the log service) 

Path where logs are generated using above service : /opt/gen_logs/logs/access.log 

Now write a flume configuration file named flumel.conf , using that configuration file dumps logs in HDFS file system in a directory called flumel. Flume channel should have following property as well. 
After every 100 message it should be committed, 
use non-durable/faster channel and it should be able to hold maximum 1000 events Solution.

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /opt/gen_logs/logs/access.log

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /user/cloudera/flume1
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1 

flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob4.conf

QUESTION 68 CORRECT TEXT Problem Scenario 23 : 
You have been given log generating service as below. 

Start_logs (It will generate continuous logs) 
Tail_logs (You can check , what logs are being generated) 
Stop_logs (It will stop the log service) 
Path where logs are generated using above service : /opt/gen_logs/logs/access.log 
Now write a flume configuration file named flume3.conf , using that configuration file dumps logs in HDFS file system in a directory called flumeflume3/%Y/%m/%d/%H/%M 
Means every minute new directory should be created). 
Please use the interceptors to provide timestamp information, if message header does not have header info. And also note that you have to preserve existing timestamp, if message contains it. Flume channel should have following property as well. After every 100 message it should be committed, use non-durable/faster channel and it should be able to hold maximum 1000 events

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /opt/gen_logs/logs/access.log
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = timestamp

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /user/cloudera/flumeflume3/%Y/%m/%d/%H/%M
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1 

flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob5.conf

QUESTION 1 CORRECT TEXT Problem Scenario 28 : You need to implement near real time solutions for collecting information when submitted in file with below 
 
Data 
 
echo "IBM,100,20160104" >> /tmp/spooldir2/.bb.txt 
echo "IBM,103,20160105" >> /tmp/spooldir2/.bb.txt 
mv /tmp/spooldir2/.bb.txt /tmp/spooldir2/bb.txt 

After few mins 
echo "IBM,100.2,20160104" >> /tmp/spooldir2/.dr.txt 
echo "IBM,103.1,20160105" >> /tmp/spooldir2/.dr.txt 
mv /tmp/spooldir2/.dr.txt /tmp/spooldir2/dr.txt 

You have been given below directory location (if not available than create it) /tmp/spooldir2 . 
As soon as file committed in this directory that needs to be available in hdfs in /tmp/flume/primary as well as /tmp/flume/secondary location. 
However, note that/tmp/flume/secondary is optional, if transaction failed which writes in this directory need not to be rollback. 
Write a flume configuration file named flumeS.conf and use it to load data in hdfs with following additional properties . 

# Name the components on this agent
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /tmp/spooldir2
a1.sources.r1.selector.type = replicating
a1.sources.r1.selector.optional = c2

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /tmp/flume/primary
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /tmp/flume/secondary
a1.sinks.k2.hdfs.fileType = DataStream
a1.sinks.k2.hdfs.writeFormat = Text

# Use a channel which buffers events in memory
a1.channels.c1.type = file
a1.channels.c1.dataDirs=/user/cloudera/c1_datadir
a1.channels.c1.dataDirs=/mnt/alpha_data/
a1.channels.c2.type = file
a1.channels.c2.dataDirs=/user/cloudera/c2_datadir
a1.channels.c2.dataDirs=/mnt/alpha_data/

# Bind the source and sink to the channel
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2

flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob6.conf

Problem 7: 
This step comprises of three substeps. Please perform tasks under each subset completely  

using sqoop pull data from MYSQL orders table into /user/cloudera/problem7/prework as AVRO data file using only one mapper

Pull the file from \user\cloudera\problem7\prework into a local folder named flume-avro

create a flume agent configuration such that it has an avro source at localhost and port number 11112,  a jdbc channel and an hdfs file sink at /user/cloudera/problem7/sink
Use the following command to run an avro client flume-ng avro-client -H localhost -p 11112 -F <<Provide your avro file path here>>

sqoop-import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username root \
--password cloudera \
--table orders \
--as-avrodatafile \
--target-dir /user/cloudera/problem7/prework1 \
--num-mappers 1


hdfs dfs -get /user/cloudera/problem7/prework1 flume-avro1


flume-ng avro-client -H localhost -p 11112 -F /home/cloudera/flume-avro1/part-m-00000.avro


a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = avro
a1.sources.r1.bind = 127.0.0.1
a1.sources.r1.port = 11112

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /user/cloudera/problem7/sink_avro2
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.serializer = avro_event
a1.sinks.k1.hdfs.fileSuffix = .avro

# Use a channel which buffers events in memory
a1.channels.c1.type = file

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob7.conf

PROBLEM FORMULA
=======================


Data dumps from a trading system is periodically downloaded to flat files at a shared location /home/cloudera/Downloads/shared_files.


Write a system that transfers all the files downloaded to the shared folder to a hdfs location /user/cloudera/output/flume/price_data with the following properties.


1.The size of each file in the final destination should not exceed one hdfs block size
.
2.Files could rollover every 5 mins

3.Each file should not contain more than 100000 records

4.All files in the final location should have their file names prefixed with trade_data
.
5.All transfered files should be in the format that they were read

6.No data loss should be allowed even with restart of the transfering system

7.Files should be read by oldest first.
8.No duplicates during transfer of data
.
9.Your submission for data transfer should be left running even after the exams ends
.
10.You can delete all files from the shared folder after they have been successfully read 




Solution 

a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/cloudera/Downloads/shared_files
a1.sources.r1.deletePolicy = immediate
a1.sources.r1.consumeOrder = oldest

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /user/cloudera/output/flume/price_data
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.rollCount = 100000
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.filePrefix = trade_data
a1.sinks.k1.hdfs.rollInterval = 300


# Use a channel which buffers events in memory
a1.channels.c1.type = file
a1.channels.c1.dataDirs = /home/cloudera/flume-avro/datadir
a1.channels.c1.checkpointDir = /home/cloudera/flume-avro/ckdir

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

flume-ng agent -n a1 -f /home/cloudera/flume-avro/flume-prob8.conf