class SparkContext(__builtin__.object)
 |  Main entry point for Spark functionality. A SparkContext represents the
 |  connection to a Spark cluster, and can be used to create L{RDD} and
 |  broadcast variables on that cluster.
 |  
 |  Methods defined here:
 |  
 |  __enter__(self)
 |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.
 |  
 |  __exit__(self, type, value, trace)
 |      Enable 'with SparkContext(...) as sc: app' syntax.
 |      
 |      Specifically stop the context on exit of the with block.
 |  
 |  __getnewargs__(self)
 |  
 |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)
 |      Create a new SparkContext. At least the master and app name should be set,
 |      either through the named parameters here or through C{conf}.
 |      
 |      :param master: Cluster URL to connect to
 |             (e.g. mesos://host:port, spark://host:port, local[4]).
 |      :param appName: A name for your job, to display on the cluster web UI.
 |      :param sparkHome: Location where Spark is installed on cluster nodes.
 |      :param pyFiles: Collection of .zip or .py files to send to the cluster
 |             and add to PYTHONPATH.  These can be paths on the local file
 |             system or HDFS, HTTP, HTTPS, or FTP URLs.
 |      :param environment: A dictionary of environment variables to set on
 |             worker nodes.
 |      :param batchSize: The number of Python objects represented as a single
 |             Java object. Set 1 to disable batching, 0 to automatically choose
 |             the batch size based on object sizes, or -1 to use an unlimited
 |             batch size
 |      :param serializer: The serializer for RDDs.
 |      :param conf: A L{SparkConf} object setting Spark properties.
 |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM
 |             will be instantiated.
 |      :param jsc: The JavaSparkContext instance (optional).
 |      :param profiler_cls: A class of custom Profiler used to do profiling
 |             (default is pyspark.profiler.BasicProfiler).
 |      
 |      
 |      >>> from pyspark.context import SparkContext
 |      >>> sc = SparkContext('local', 'test')
 |      
 |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL
 |      Traceback (most recent call last):
 |          ...
 |      ValueError:...
 |  
 |  accumulator(self, value, accum_param=None)
 |      Create an L{Accumulator} with the given initial value, using a given
 |      L{AccumulatorParam} helper object to define how to add values of the
 |      data type if provided. Default AccumulatorParams are used for integers
 |      and floating-point numbers if you do not provide one. For other types,
 |      a custom AccumulatorParam can be used.
 |  
 |  addFile(self, path)
 |      Add a file to be downloaded with this Spark job on every node.
 |      The C{path} passed can be either a local file, a file in HDFS
 |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or
 |      FTP URI.
 |      
 |      To access the file in Spark jobs, use
 |      L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the
 |      filename to find its download location.
 |      
 |      >>> from pyspark import SparkFiles
 |      >>> path = os.path.join(tempdir, "test.txt")
 |      >>> with open(path, "w") as testFile:
 |      ...    _ = testFile.write("100")
 |      >>> sc.addFile(path)
 |      >>> def func(iterator):
 |      ...    with open(SparkFiles.get("test.txt")) as testFile:
 |      ...        fileVal = int(testFile.readline())
 |      ...        return [x * fileVal for x in iterator]
 |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()
 |      [100, 200, 300, 400]
 |  
 |  addPyFile(self, path)
 |      Add a .py or .zip dependency for all tasks to be executed on this
 |      SparkContext in the future.  The C{path} passed can be either a local
 |      file, a file in HDFS (or other Hadoop-supported filesystems), or an
 |      HTTP, HTTPS or FTP URI.
 |  
 |  binaryFiles(self, path, minPartitions=None)
 |      .. note:: Experimental
 |      
 |      Read a directory of binary files from HDFS, a local file system
 |      (available on all nodes), or any Hadoop-supported file system URI
 |      as a byte array. Each file is read as a single record and returned
 |      in a key-value pair, where the key is the path of each file, the
 |      value is the content of each file.
 |      
 |      Note: Small files are preferred, large file is also allowable, but
 |      may cause bad performance.
 |  
 |  binaryRecords(self, path, recordLength)
 |      .. note:: Experimental
 |      
 |      Load data from a flat binary file, assuming each record is a set of numbers
 |      with the specified numerical format (see ByteBuffer), and the number of
 |      bytes per record is constant.
 |      
 |      :param path: Directory to the input data files
 |      :param recordLength: The length at which to split the records
 |  
 |  broadcast(self, value)
 |      Broadcast a read-only variable to the cluster, returning a
 |      L{Broadcast<pyspark.broadcast.Broadcast>}
 |      object for reading it in distributed functions. The variable will
 |      be sent to each cluster only once.
 |  
 |  cancelAllJobs(self)
 |      Cancel all jobs that have been scheduled or are running.
 |  
 |  cancelJobGroup(self, groupId)
 |      Cancel active jobs for the specified group. See L{SparkContext.setJobGroup}
 |      for more information.
 |  
 |  clearFiles(self)
 |      Clear the job's list of files added by L{addFile} or L{addPyFile} so
 |      that they do not get downloaded to any new nodes.
 |  
 |  dump_profiles(self, path)
 |      Dump the profile stats into directory `path`
 |  
 |  emptyRDD(self)
 |      Create an RDD that has no partitions or elements.
 |  
 |  getLocalProperty(self, key)
 |      Get a local property set in this thread, or null if it is missing. See
 |      L{setLocalProperty}
 |  
 |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)
 |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,
 |      a local file system (available on all nodes), or any Hadoop-supported file system URI.
 |      The mechanism is the same as for sc.sequenceFile.
 |      
 |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a
 |      Configuration in Java.
 |      
 |      :param path: path to Hadoop file
 |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat
 |             (e.g. "org.apache.hadoop.mapred.TextInputFormat")
 |      :param keyClass: fully qualified classname of key Writable class
 |             (e.g. "org.apache.hadoop.io.Text")
 |      :param valueClass: fully qualified classname of value Writable class
 |             (e.g. "org.apache.hadoop.io.LongWritable")
 |      :param keyConverter: (None by default)
 |      :param valueConverter: (None by default)
 |      :param conf: Hadoop configuration, passed in as a dict
 |             (None by default)
 |      :param batchSize: The number of Python objects represented as a single
 |             Java object. (default 0, choose batchSize automatically)
 |  
 |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)
 |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary
 |      Hadoop configuration, which is passed in as a Python dict.
 |      This will be converted into a Configuration in Java.
 |      The mechanism is the same as for sc.sequenceFile.
 |      
 |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat
 |             (e.g. "org.apache.hadoop.mapred.TextInputFormat")
 |      :param keyClass: fully qualified classname of key Writable class
 |             (e.g. "org.apache.hadoop.io.Text")
 |      :param valueClass: fully qualified classname of value Writable class
 |             (e.g. "org.apache.hadoop.io.LongWritable")
 |      :param keyConverter: (None by default)
 |      :param valueConverter: (None by default)
 |      :param conf: Hadoop configuration, passed in as a dict
 |             (None by default)
 |      :param batchSize: The number of Python objects represented as a single
 |             Java object. (default 0, choose batchSize automatically)
 |  
 |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)
 |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,
 |      a local file system (available on all nodes), or any Hadoop-supported file system URI.
 |      The mechanism is the same as for sc.sequenceFile.
 |      
 |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a
 |      Configuration in Java
 |      
 |      :param path: path to Hadoop file
 |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat
 |             (e.g. "org.apache.hadoop.mapreduce.lib.input.TextInputFormat")
 |      :param keyClass: fully qualified classname of key Writable class
 |             (e.g. "org.apache.hadoop.io.Text")
 |      :param valueClass: fully qualified classname of value Writable class
 |             (e.g. "org.apache.hadoop.io.LongWritable")
 |      :param keyConverter: (None by default)
 |      :param valueConverter: (None by default)
 |      :param conf: Hadoop configuration, passed in as a dict
 |             (None by default)
 |      :param batchSize: The number of Python objects represented as a single
 |             Java object. (default 0, choose batchSize automatically)
 |  
 |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)
 |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary
 |      Hadoop configuration, which is passed in as a Python dict.
 |      This will be converted into a Configuration in Java.
 |      The mechanism is the same as for sc.sequenceFile.
 |      
 |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat
 |             (e.g. "org.apache.hadoop.mapreduce.lib.input.TextInputFormat")
 |      :param keyClass: fully qualified classname of key Writable class
 |             (e.g. "org.apache.hadoop.io.Text")
 |      :param valueClass: fully qualified classname of value Writable class
 |             (e.g. "org.apache.hadoop.io.LongWritable")
 |      :param keyConverter: (None by default)
 |      :param valueConverter: (None by default)
 |      :param conf: Hadoop configuration, passed in as a dict
 |             (None by default)
 |      :param batchSize: The number of Python objects represented as a single
 |             Java object. (default 0, choose batchSize automatically)
 |  
 |  parallelize(self, c, numSlices=None)
 |      Distribute a local Python collection to form an RDD. Using xrange
 |      is recommended if the input represents a range for performance.
 |      
 |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()
 |      [[0], [2], [3], [4], [6]]
 |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()
 |      [[], [0], [], [2], [4]]
 |  
 |  pickleFile(self, name, minPartitions=None)
 |      Load an RDD previously saved using L{RDD.saveAsPickleFile} method.
 |      
 |      >>> tmpFile = NamedTemporaryFile(delete=True)
 |      >>> tmpFile.close()
 |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)
 |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())
 |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
 |  
 |  range(self, start, end=None, step=1, numSlices=None)
 |      Create a new RDD of int containing elements from `start` to `end`
 |      (exclusive), increased by `step` every element. Can be called the same
 |      way as python's built-in range() function. If called with a single argument,
 |      the argument is interpreted as `end`, and `start` is set to 0.
 |      
 |      :param start: the start value
 |      :param end: the end value (exclusive)
 |      :param step: the incremental step (default: 1)
 |      :param numSlices: the number of partitions of the new RDD
 |      :return: An RDD of int
 |      
 |      >>> sc.range(5).collect()
 |      [0, 1, 2, 3, 4]
 |      >>> sc.range(2, 4).collect()
 |      [2, 3]
 |      >>> sc.range(1, 7, 2).collect()
 |      [1, 3, 5]
 |  
 |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)
 |      Executes the given partitionFunc on the specified set of partitions,
 |      returning the result as an array of elements.
 |      
 |      If 'partitions' is not specified, this will run over all partitions.
 |      
 |      >>> myRDD = sc.parallelize(range(6), 3)
 |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])
 |      [0, 1, 4, 9, 16, 25]
 |      
 |      >>> myRDD = sc.parallelize(range(6), 3)
 |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)
 |      [0, 1, 16, 25]
 |  
 |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)
 |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,
 |      a local file system (available on all nodes), or any Hadoop-supported file system URI.
 |      The mechanism is as follows:
 |      
 |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key
 |             and value Writable classes
 |          2. Serialization is attempted via Pyrolite pickling
 |          3. If this fails, the fallback is to call 'toString' on each key and value
 |          4. C{PickleSerializer} is used to deserialize pickled objects on the Python side
 |      
 |      :param path: path to sequncefile
 |      :param keyClass: fully qualified classname of key Writable class
 |             (e.g. "org.apache.hadoop.io.Text")
 |      :param valueClass: fully qualified classname of value Writable class
 |             (e.g. "org.apache.hadoop.io.LongWritable")
 |      :param keyConverter:
 |      :param valueConverter:
 |      :param minSplits: minimum splits in dataset
 |             (default min(2, sc.defaultParallelism))
 |      :param batchSize: The number of Python objects represented as a single
 |             Java object. (default 0, choose batchSize automatically)
 |  
 |  setCheckpointDir(self, dirName)
 |      Set the directory under which RDDs are going to be checkpointed. The
 |      directory must be a HDFS path if running on a cluster.
 |  
 |  setJobGroup(self, groupId, description, interruptOnCancel=False)
 |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a
 |      different value or cleared.
 |      
 |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.
 |      Application programmers can use this method to group all those jobs together and give a
 |      group description. Once set, the Spark web UI will associate such jobs with this group.
 |      
 |      The application can use L{SparkContext.cancelJobGroup} to cancel all
 |      running jobs in this group.
 |      
 |      >>> import threading
 |      >>> from time import sleep
 |      >>> result = "Not Set"
 |      >>> lock = threading.Lock()
 |      >>> def map_func(x):
 |      ...     sleep(100)
 |      ...     raise Exception("Task should have been cancelled")
 |      >>> def start_job(x):
 |      ...     global result
 |      ...     try:
 |      ...         sc.setJobGroup("job_to_cancel", "some description")
 |      ...         result = sc.parallelize(range(x)).map(map_func).collect()
 |      ...     except Exception as e:
 |      ...         result = "Cancelled"
 |      ...     lock.release()
 |      >>> def stop_job():
 |      ...     sleep(5)
 |      ...     sc.cancelJobGroup("job_to_cancel")
 |      >>> supress = lock.acquire()
 |      >>> supress = threading.Thread(target=start_job, args=(10,)).start()
 |      >>> supress = threading.Thread(target=stop_job).start()
 |      >>> supress = lock.acquire()
 |      >>> print(result)
 |      Cancelled
 |      
 |      If interruptOnCancel is set to true for the job group, then job cancellation will result
 |      in Thread.interrupt() being called on the job's executor threads. This is useful to help
 |      ensure that the tasks are actually stopped in a timely manner, but is off by default due
 |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.
 |  
 |  setLocalProperty(self, key, value)
 |      Set a local property that affects jobs submitted from this thread, such as the
 |      Spark fair scheduler pool.
 |  
 |  setLogLevel(self, logLevel)
 |      Control our logLevel. This overrides any user-defined log settings.
 |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
 |  
 |  show_profiles(self)
 |      Print the profile stats to stdout
 |  
 |  sparkUser(self)
 |      Get SPARK_USER for user who is running SparkContext.
 |  
 |  statusTracker(self)
 |      Return :class:`StatusTracker` object
 |  
 |  stop(self)
 |      Shut down the SparkContext.
 |  
 |  textFile(self, name, minPartitions=None, use_unicode=True)
 |      Read a text file from HDFS, a local file system (available on all
 |      nodes), or any Hadoop-supported file system URI, and return it as an
 |      RDD of Strings.
 |      
 |      If use_unicode is False, the strings will be kept as `str` (encoding
 |      as `utf-8`), which is faster and smaller than unicode. (Added in
 |      Spark 1.2)
 |      
 |      >>> path = os.path.join(tempdir, "sample-text.txt")
 |      >>> with open(path, "w") as testFile:
 |      ...    _ = testFile.write("Hello world!")
 |      >>> textFile = sc.textFile(path)
 |      >>> textFile.collect()
 |      [u'Hello world!']
 |  
 |  union(self, rdds)
 |      Build the union of a list of RDDs.
 |      
 |      This supports unions() of RDDs with different serialized formats,
 |      although this forces them to be reserialized using the default
 |      serializer:
 |      
 |      >>> path = os.path.join(tempdir, "union-text.txt")
 |      >>> with open(path, "w") as testFile:
 |      ...    _ = testFile.write("Hello")
 |      >>> textFile = sc.textFile(path)
 |      >>> textFile.collect()
 |      [u'Hello']
 |      >>> parallelized = sc.parallelize(["World!"])
 |      >>> sorted(sc.union([textFile, parallelized]).collect())
 |      [u'Hello', 'World!']
 |  
 |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)
 |      Read a directory of text files from HDFS, a local file system
 |      (available on all nodes), or any  Hadoop-supported file system
 |      URI. Each file is read as a single record and returned in a
 |      key-value pair, where the key is the path of each file, the
 |      value is the content of each file.
 |      
 |      If use_unicode is False, the strings will be kept as `str` (encoding
 |      as `utf-8`), which is faster and smaller than unicode. (Added in
 |      Spark 1.2)
 |      
 |      For example, if you have the following files::
 |      
 |        hdfs://a-hdfs-path/part-00000
 |        hdfs://a-hdfs-path/part-00001
 |        ...
 |        hdfs://a-hdfs-path/part-nnnnn
 |      
 |      Do C{rdd = sparkContext.wholeTextFiles("hdfs://a-hdfs-path")},
 |      then C{rdd} contains::
 |      
 |        (a-hdfs-path/part-00000, its content)
 |        (a-hdfs-path/part-00001, its content)
 |        ...
 |        (a-hdfs-path/part-nnnnn, its content)
 |      
 |      NOTE: Small files are preferred, as each file will be loaded
 |      fully in memory.
 |      
 |      >>> dirPath = os.path.join(tempdir, "files")
 |      >>> os.mkdir(dirPath)
 |      >>> with open(os.path.join(dirPath, "1.txt"), "w") as file1:
 |      ...    _ = file1.write("1")
 |      >>> with open(os.path.join(dirPath, "2.txt"), "w") as file2:
 |      ...    _ = file2.write("2")
 |      >>> textFiles = sc.wholeTextFiles(dirPath)
 |      >>> sorted(textFiles.collect())
 |      [(u'.../1.txt', u'1'), (u'.../2.txt', u'2')]
 |  
 |  ----------------------------------------------------------------------
 |  Class methods defined here:
 |  
 |  getOrCreate(cls, conf=None) from __builtin__.type
 |      Get or instantiate a SparkContext and register it as a singleton object.
 |      
 |      :param conf: SparkConf (optional)
 |  
 |  setSystemProperty(cls, key, value) from __builtin__.type
 |      Set a Java system property, such as spark.executor.memory. This must
 |      must be invoked before instantiating SparkContext.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  applicationId
 |      A unique identifier for the Spark application.
 |      Its format depends on the scheduler implementation.
 |      
 |      * in case of local spark app something like 'local-1433865536131'
 |      * in case of YARN something like 'application_1433865536131_34483'
 |      
 |      >>> sc.applicationId  # doctest: +ELLIPSIS
 |      u'local-...'
 |  
 |  defaultMinPartitions
 |      Default min number of partitions for Hadoop RDDs when not given by user
 |  
 |  defaultParallelism
 |      Default level of parallelism to use when not given by user (e.g. for
 |      reduce tasks)
 |  
 |  startTime
 |      Return the epoch time when the Spark Context was started.
 |  
 |  version
 |      The version of Spark on which this application is running.
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes defined here:
 |  
 |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')




*************************************************************************************

>>> rdd=sc.textFile('/public/retail_db/order_items/part-00000')
rdd.getNumPartitions()


l=open("/data/retail_db/orders/part-00000")
lRdd=sc.parallelize(l)
lRdd.getNumPartitions()

pyspark --master yarn --conf spark.ui.port=12888 --num-executors 4

rdd=sqlContext.read.json("/public/retail_db_json/customers/part-r-00000-70554560-527b-44f6-9e80-4e2031af5994").show()

rdd=sqlContext.load("/public/retail_db_json/customers/part-r-00000-70554560-527b-44f6-9e80-4e2031af5994","json").show()

rdd.map(lambda x:int(x.split(',')[1].split(' ')[0].replace('-',''))).first()

rdd.map(lambda x:int(x.split(',')[1].split(' ')[0].replace('-',''))).take(1)
rdd.map(lambda x:int(x.split(',')[1].split(' ')[0].replace('-',''))).count()
countRdd=rdd.map(lambda x:(x.split(',')[3],1)).reduceByKey(lambda x,y:x+y)

********************24 Nov******************************************
ordersRDD=sc.textFile("/public/retail_db/orders/part-00000").map(lambda x:(int(x.split(',')[0]),x.split(',')[1]))
ordersRDD.take(2)

oiRDD=oiRDD.map(lambda x:(int(x[0]),float(x[1])))

oiRDD=sc.textFile("/public/retail_db/order_items/part-00000").map(lambda x:(x.split(',')[1],x.split(',')[5]))

oiRDD=oiRDD.map(lambda x:(int(x[0]),float(x[1])))

oiRDD.take(2)

frdd=ordersRDD.join(oiRDD)
frdd.take(2)

frdd=ordersRDD.leftOuterJoin(oiRDD)

frdd=ordersRDD.leftOuterJoin(oiRDD).filter(lambda x:x[0]==65596)

ordersRDD.leftOuterJoin(oiRDD).filter(lambda x:x[0]==65596).collect()

frdd=ordersRDD.leftOuterJoin(oiRDD).filter(lambda x:x[1][1]==None)
for i in frdd.take(100):
...  print(i)

#Get revenue for a given order_id

oiRDD=sc.textFile("/public/retail_db/order_items").map(lambda x:(int(x.split(',')[1]),float(x.split(',')[4])))

.reduce(lambda x,y:x[1]+y[1])


.map(lambda x:(int(x.split(',')[0]),float(x.split(',')[4])))

 oiRDD=oiRDD.filter(lambda x:int(x.split(',')[1])==2)

**********************25 Nov*****************************************
#Count orders by status
ordersRDD=sc.textFile("/public/retail_db/orders").map(lambda x:(x.split(',')[3],1)).countByKey()

#get revenue for each order_id using groupByKey
oiRDD=sc.textFile("/public/retail_db/order_items").map(lambda x:(int(x.split(',')[1]),float(x.split(',')[4])))
oiRDD.take(2)


oiRDD=oiRDD.groupByKey().filter(lambda x:x[0]==2).map(lambda x:list(x[1]))
oiRDD=oiRDD.groupByKey()
l=oiRDD.first()
sum(list(l[1]))

oiRDD=oiRDD.groupByKey().map(lambda x:(x[0],sum(x[1]))

#Get order item details in descending order by revenue - groupByKey
oiRDD=sc.textFile("/public/retail_db/order_items") \
      .map(lambda x:(int(x.split(',')[1]),float(x.split(',')[4])))
oiRDD1=oiRDD.groupByKey().map(lambda x:(x[0],sorted(x[1],reverse=True)))
oiRDD1.take(5)


oiRDD=sc.textFile("/public/retail_db/order_items").map(lambda x:(int(x.split(',')[1]),x))
oiRDD1=oiRDD.groupByKey()
oiRDD1.take(5)
#oiRDD2=oiRDD1.map(lambda x:list(x[1]))
oiRDD2=oiRDD1.flatMap(lambda x:sorted(list(x[1]),key=lambda k:k.split(',')[4],reverse=True))
oiRDD2.take(5)

for i in oiRDD2.take(10):
 print(i)
